---
title: "HW2"
author: "黃亞婕"
date: "2021/3/25"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 4
    number_sections: false
    toc_float: true
---
```{r, message = FALSE, results='hide'}
library(rstan)
library(rethinking)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(loo)
```

## Q1.

Spurious correlation: Students that spend many time studying tend to have whiter skin color. 

#### a) Study Time 和 Outdoor Time

```{r, message = FALSE, results='hide'}
N <- 100
study_T <- rnorm(n = N, mean = 0, sd = 1)
outdoor_T <- rnorm(n = N, mean = study_T, sd = 1)
skin <- rnorm(n = N, mean = study_T, sd = 3)
d <- data.frame(study_T, outdoor_T, skin)

mdl1.1 = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real alpha;
    real beta1;
    real<lower = 0> sigma;
}
model {
    alpha ~ normal(0,5);
    beta1 ~ normal(0,1);
    sigma ~ normal(0,1);
    
    y ~ normal(alpha + x * beta1, sigma);
}
"
lm_data1.1 = list(N = nrow(d) , x = d$study_T , y = d$skin)
fit1.1 = stan(model_code = mdl1.1 , data = lm_data1.1 , iter = 2000 , chains = 2 , cores = 2)
```

#### b) Skin Color 和 Outdoor Time

```{r, message = FALSE, results='hide'}
mdl1.2 = "
data {
    int N;
    vector[N] x;
    vector[N] y;
}
parameters {
    real alpha;
    real beta2;
    real<lower = 0> sigma;
}
model {
    alpha ~ normal(0,5);
    beta2 ~ normal(0,1);
    sigma ~ normal(0,1);

    y ~ normal(alpha + x * beta2, sigma);
}
"
lm_data1.2 = list(N = nrow(d) , x = d$outdoor_T , y = d$skin)
fit1.2 = stan(model_code = mdl1.2, data = lm_data1.2)
```

#### c) Study time 和 Outdoor Time 同時加入 model

```{r, message = FALSE, results='hide'}
mdl1.3 = "
data {
    int N;
    vector[N] x1;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real alpha;
    real beta1;
    real beta2;
    real<lower = 0> sigma;
}
model {
    alpha ~ normal(0,5);
    beta1 ~ normal(0,1);
    beta2 ~ normal(0,1);
    sigma ~ normal(0,1);

    y ~ normal(alpha + x1 * beta1 + x2 * beta2, sigma);
}
"
lm_data1.3 = list(N = nrow(d) , x1 = d$study_T , x2 = d$outdoor_T , y = d$skin)
fit1.3 = stan(model_code = mdl1.3, data = lm_data1.3)
```

#### Plot out the results using Coefficient Table
```{r}
plot(coeftab(fit1.1 , fit1.2 , fit1.3), pars=c("beta1","beta2"))
```

This is an example of spurious correlation as we can see that the median decreased when both predictors entered into the same model.

## Q2. 

**Performance from the amount of help received from mentors and the number of days they took off.**

首先建立 simulate masked relationship 的模型，`x_pos` 是 the amount of help received from mentors，`x_neg` 是 the number of days they took off，並預測 performance。

```{r}
N = 100
rho = 0.7
x_pos = rnorm(n = N, mean = 0, sd = 1)
x_neg = rnorm(n = N, mean = rho * x_pos, sd = sqrt(1 - rho^2))
perf = rnorm(n = N , mean = x_pos - x_neg , sd = 1)
d = data.frame( perf , x_pos , x_neg)
```

#### a) Amount of help vs performance
```{r, message = FALSE, results='hide'}
mdl2.1 = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real alpha;
    real beta1;
    real<lower = 0> sigma;
}
model {
    alpha ~ normal(0,5);
    beta1 ~ normal(0,1);
    sigma ~ uniform(0,2);
    
    y ~ normal(alpha + x * beta1, sigma);
}
"
lm_data2.1 = list(N = nrow(d) , x = d$x_pos , y = d$perf)
fit2.1 = stan(model_code = mdl2.1 , data = lm_data2.1 , iter = 2000 , chains = 2 , cores = 2)
```

#### b) The days they took off vs performance
```{r, message = FALSE, results='hide'}
mdl2.2 = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real alpha;
    real beta2;
    real<lower = 0> sigma;
}
model {
    alpha ~ normal(0,5);
    beta2 ~ normal(0,1);
    sigma ~ uniform(0,2);
    
    y ~ normal(alpha + x * beta2, sigma);
}
"
lm_data2.2 = list(N = nrow(d) , x = d$x_neg , y = d$perf)
fit2.2 = stan(model_code = mdl2.2 , data = lm_data2.2 , iter = 2000 , chains = 2 , cores = 2)
```

#### c) Put both variable in the model
```{r, message = FALSE, results='hide'}
mdl2.3 = "
data {
    int N;
    vector[N] x1;
    vector[N] x2;
    vector[N] y;
}
parameters {
    real alpha;
    real beta1;
    real beta2;
    real<lower = 0> sigma;
}
model {
    alpha ~ normal(0,5);
    beta1 ~ normal(0,1);
    beta2 ~ normal(0,1);
    sigma ~ normal(0,1);

    y ~ normal(alpha + x1 * beta1 + x2 * beta2, sigma);
}
"
lm_data2.3 = list(N = nrow(d) , x1 = d$x_pos , x2 = d$x_neg , y = d$perf)
fit2.3 = stan(model_code = mdl2.3, data = lm_data2.3)
```

Compare the correlation of the three models
```{r}
precis(fit2.1)
precis(fit2.2)
precis(fit2.3)
```

We can see that when the variables are separate in different models, amount of help present a 正相關 with performance and the days they took off shows 負相關。However, the associations becomes higher when both variables were added to the model.

#### Plot out the results using Coefficient Table
```{r}
plot(coeftab(fit2.1 , fit2.2 , fit2.3), pars=c("beta1","beta2"))
```

From all the above data, this is an example of a masked relationship.

## Q3.

首先將 `area` `avgfood` `groupsize` 等欄位標準化，創造新的 Date Frame `new_fox`。

```{r}
data(foxes)

new_fox = foxes
new_fox$std_area = as.vector(scale(new_fox$area))
new_fox$std_avgfood = as.vector(scale(new_fox$avgfood))
new_fox$std_groupsize = as.vector(scale(new_fox$groupsize))
```

#### Fit the model

##### (1) Area v.s. Weight
```{r, message = FALSE, results='hide'}
mdl3.1 = "
data {
  int N;
  vector[N] weight;
  vector[N] area;
}
parameters {
  real alpha;
  real betaA;
  real sigma;
}
model {
  
  weight ~ normal(alpha + betaA * area, sigma);
  
  alpha ~ normal(0,10);
  betaA ~ normal(0,1);
  sigma ~ uniform(0,5);
}

generated quantities {
    real pred_weight[N];
    vector[N] mu = alpha + betaA * area;

    mu = alpha + betaA * area;
    pred_weight = normal_rng(mu, sigma);
}
"
lm_data3.1 = list(N = nrow(new_fox) , weight = new_fox$weight , area = new_fox$std_area)
fit3.1 = stan(model_code = mdl3.1, data = lm_data3.1, iter = 1000, cores = 4, chains = 2)
```

##### (2) Group Size v.s. weight
```{r, message = FALSE, results='hide'}
mdl3.2 = "
data {
  int N;
  vector[N] weight;
  vector[N] groupsize;
}
parameters {
  real alpha;
  real betaG;
  real sigma;
}
model {
  
  weight ~ normal(alpha + betaG * groupsize, sigma);
  
  alpha ~ normal(0,10);
  betaG ~ normal(0,1);
  sigma ~ uniform(0,5);
}

generated quantities {
    real pred_weight[N];
    vector[N] mu = alpha + betaG * groupsize;

    mu = alpha + betaG * groupsize;
    pred_weight = normal_rng(mu, sigma);
}
"
lm_data3.2 = list(N = nrow(new_fox) , weight = new_fox$weight , groupsize = new_fox$std_groupsize)
fit3.2 = stan(model_code = mdl3.2, data = lm_data3.2, iter = 1000, cores = 4, chains = 2)
```


#### Plot out the results

Regression line and the 95% interval of the mean

```{r}
# Area v.s. Weight
frame3.1 = as.data.frame(fit3.1)

pred_mu = frame3.1 %>% 
  select(contains("mu"))

PI = data.frame(
  mean = pred_mu %>% apply(., 2, mean),
  L_HPDI = pred_mu %>% apply(. , 2 , PI , prob = 0.95) %>% .[1,],
  H_HPDI = pred_mu %>% apply(. , 2 , PI , prob = 0.95) %>% .[2,],
  area = new_fox$std_area)

pic1 = PI %>% ggplot() +
  geom_point(data = new_fox, aes(std_area, weight), color="blue", alpha = 0.3) +
  geom_line(aes(area , mean)) +
  geom_ribbon(aes(x = area , ymin = L_HPDI , ymax = H_HPDI), alpha = 0.3) +
  ggtitle("Regression Line")+
  labs(y="Predicted Weight", x = "Area")

# Group Size v.s. Weight
frame3.2 = as.data.frame(fit3.2)

pred_mu = frame3.2 %>% 
  select(contains("mu"))

PI = data.frame(
  mean = pred_mu %>% apply(., 2, mean),
  L_HPDI = pred_mu %>% apply(. , 2 , PI , prob = 0.95) %>% .[1,],
  H_HPDI = pred_mu %>% apply(. , 2 , PI , prob = 0.95) %>% .[2,],
  groupsize = new_fox$std_groupsize)

pic2 = PI %>% ggplot() +
  geom_point(data = new_fox, aes(std_groupsize, weight), color="blue", alpha = 0.3) +
  geom_line(aes(groupsize , mean)) +
  geom_ribbon(aes(x = groupsize , ymin = L_HPDI , ymax = H_HPDI), alpha = 0.3) +
  ggtitle("Regression Line")+
  labs(y = "Predicted Weight", x = "Group Size")

grid.arrange(pic1 , pic2 , nrow = 1)
```

#### Interpretation
由上面兩張圖的回歸線來看，Territory Area 基本上為水平線，Group Size 雖然有些微的負斜率，但對 Weight 的預測沒有實際效果，因此兩者都對 Weight 的預測不是很重要。

## Q4.

#### Fit the model

```{r , message = FALSE, results='hide'}
mdl4 = "
data {
    int N;
    vector[N] x1;
    vector[N] x2;
    vector[N] y;
    vector[N] c1;
    vector[N] c2;
}
parameters {
    real alpha;
    real betaA;
    real betaG;
    real<lower = 0> sigma;
}
model {
    alpha ~ normal(0,10);
    betaA ~ normal(0,1);
    betaG ~ normal(0,1);
    sigma ~ normal(0,5);

    y ~ normal(alpha + x1 * betaA + x2 * betaG, sigma);
}
generated quantities {
    real pred_w1[N];
    real pred_w2[N];
    vector[N] mu1;
    vector[N] mu2;

    mu1 = alpha + betaA * 0 + betaG * c2;
    pred_w1 = normal_rng(mu1, sigma);
    
    mu2 = alpha + betaA * c1 + betaG * 0;
    pred_w2 = normal_rng(mu2, sigma);
}
"
lm_data4 = list(N = nrow(new_fox) , x1 = new_fox$std_area , x2 = new_fox$std_groupsize , y = new_fox$weight , c1 = seq(-3, 3, length.out = 116) , c2 = seq(-3, 3, length.out = 116))
fit4 = stan(model_code = mdl4, data = lm_data4, iter = 1000, cores = 4, chains = 2)
```

#### Plot out the results

```{r}
frame4 = as.data.frame(fit4)

# Counterfactual: fixed area
pred_mu_1 = frame4 %>% select(contains("mu1"))

CF_A = data.frame(
  x = seq(-3, 3, length.out = 116),
  pred = pred_mu_1 %>% apply(., 2, mean),
  CI_lower = pred_mu_1 %>% apply(. , 2, HPDI) %>% .[1,],
  CI_upper = pred_mu_1 %>% apply(. , 2, HPDI) %>% .[2,]
)

p_CFA = CF_A %>% 
  ggplot() +
  geom_point(data = new_fox, aes(std_area, weight), color="blue", alpha = 0.3)+
  geom_line(aes(x, pred)) +
  geom_ribbon(aes(x , ymin = CI_lower, ymax = CI_upper), alpha = 0.3) +
  labs(x = "Group size", y = "Predicted Weight")

# Counterfactual: fixed Groupsize
pred_mu_2 = frame4 %>% select(contains("mu2"))

CF_B = data.frame(
  x = seq(-3 , 3, length.out = 116),
  pred = pred_mu_2 %>% apply(. , 2, mean),
  CI_lower = pred_mu_2 %>% apply(. , 2, HPDI) %>% .[1,],
  CI_upper = pred_mu_2 %>% apply(. , 2, HPDI) %>% .[2,]
)

p_CFB = CF_B %>% 
  ggplot() +
  geom_point(data = new_fox, aes(std_groupsize, weight), color = "blue", alpha = 0.3) +
  geom_line(aes(x, pred)) +
  geom_ribbon(aes(x , ymin = CI_lower, ymax = CI_upper), alpha = 0.3) +
  labs(x = "Area", y = "Predicted Weight")

grid.arrange(p_CFA , p_CFB , nrow = 1)
```

#### Interpretation

由上面兩張圖可看出 Area 和 Groupsize 都對 Predicted Weight 有很高的重要性，因兩個變數有 masked  relationship ， 所以與第三題得到的結果不同。

## Q5.

#### (1) Avgfood and Groupsize as 變因

```{r , message = FALSE, results='hide'}
#body weight as an additive function of avgfood and groupsize
mdl5.1 = "
data {
  int N;
  vector[N] x1;
  vector[N] x2;
  vector[N] y;
  vector[N] c1;
  vector[N] c2;
}
parameters {
  real alpha;
  real betaAF;
  real betaG;
  real sigma;
}
model {
  vector[N] mu = alpha + betaAF * x1 + betaG * x2;

  y ~ normal(mu, sigma);
  alpha ~ normal(0,10);
  betaAF ~ normal(0,1);
  betaG ~ normal(0,1);
  sigma ~ uniform(0,5);
}
generated quantities {
    real pred_w1[N];
    real pred_w2[N];
    vector[N] mu1;
    vector[N] mu2;

    mu1 = alpha + betaAF * 0 + betaG * c2;
    pred_w1 = normal_rng(mu1, sigma);
    
    mu2 = alpha + betaAF * c1 + betaG * 0;
    pred_w2 = normal_rng(mu2, sigma);
}
"
lm_data5.1 = list(N = nrow(new_fox) , x1 = new_fox$std_avgfood , x2 = new_fox$std_groupsize , y = new_fox$weight , c1 = seq(-3, 3, length.out = 116) , c2 = seq(-3, 3, length.out = 116))
fit5.1 = stan(model_code = mdl5.1, data = lm_data5.1, iter = 1000, cores = 4, chains = 2)
```

#### (2) Avgfood and Groupsize and area 為變因

```{r , message = FALSE, results='hide'}
#body weight as an additive function of all three variables, avgfood and groupsize and area
mdl5.2 = "
data {
  int N;
  vector[N] avgfood;
  vector[N] groupsize;
  vector[N] weight;
  vector[N] area;
}
parameters {
  real alpha;
  real betaAF;
  real betaA;
  real betaG;
  real sigma;
}
model {
  vector[N] mu = alpha + betaAF * avgfood + betaA * area + betaG * groupsize;

  weight ~ normal(mu, sigma);
  alpha ~ normal(0,10);
  betaAF ~ normal(0,1);
  betaG ~ normal(0,1);
  betaA ~ normal(0,1);
  sigma ~ uniform(0,5);
}
"
lm_data5.2 = list(N = nrow(new_fox) , avgfood = new_fox$std_avgfood , groupsize = new_fox$std_groupsize , area = new_fox$std_area , weight = new_fox$weight)
fit5.2 = stan(model_code = mdl5.2, data = lm_data5.2, iter = 2000, cores = 4, chains = 2)
```

#### (a) Is avgfood or area a better predictor of body weight? 

Plot out the regression model (mdl5.1) and compare it to the plot (p_CFB) from Q4.

```{r}
frame5.1 = as.data.frame(fit5.1)
pred_mu_2 = frame5.1 %>% select(contains("mu2"))

CF_G = data.frame(
  x = seq(-3 , 3, length.out = 116),
  pred = pred_mu_2 %>% apply(. , 2, mean),
  CI_lower = pred_mu_2 %>% apply(. , 2, HPDI) %>% .[1,],
  CI_upper = pred_mu_2 %>% apply(. , 2, HPDI) %>% .[2,]
)

p_CFG = CF_G %>% 
  ggplot() +
  geom_point(data = new_fox, aes(std_groupsize, weight), color = "blue", alpha = 0.3) +
  geom_line(aes(x, pred)) +
  geom_ribbon(aes(x , ymin = CI_lower, ymax = CI_upper), alpha = 0.3) +
  labs(x = "Groupsize", y = "Predicted Weight")

grid.arrange(p_CFG , p_CFB , nrow = 1)
```

The estimated effect of avgfood is a little better than area. The predicted body weight in plot (avgfood) goes above 6kg at the right side, on the other hand, the plot for area didn't get that high. This suggests avgfood is a better predictor.

#### (b) Explain the results

This result is due to the multicollinearity effect between avgfood and area. The two variables are highly correlated and therefore will make the regression model not so accurate. The coefficient may change a lot even under minimal change of the data.

## Q6.

Model selection choose the model that performs the best. In model selection, it discards the information about model uncertainty, thereby lose information about differences among models. For example, if there are two models that were very similar on performance metric but different in structure, selecting one of them discard part of the evidence.

Model comparison use various model to access the accuracy and evaluate the influence of different parameters in each models. This can preserve more information and make more comprehensive judgments about the data.

## Q7.

When one model is fit to different numbers of observations, it will be judged on a different target. When fewer observations are used, the model usually performs better since the deviance will be smaller. Because there are less predict, there will be less prediction error. When all other criterion being the same, the more observations model will have higher deviance, making model accuracy lower. Based on the above reasons, all models have to fit to exactly the same observation when comparing models.

## Q8.

WAIC :  -2( lppd - p$_{WAIC}$) = -2( lppd - $\sum_{i=1}^{N} V(y_i)$ )  ，其中 $\sum_{i=1}^{N} V(y_i)$ 是 penalty term。

When in log-probabilities, smaller variances results in lower penalty. So if the prior become more concentrated, the plausible range of the parameters and the variability in the posterior distribution is decreased. As the parameters become more consistent, the log probability of each observation will also become more consistent. Therefore, the penalty will be smaller.

PSIS : P$_{D}$ is the number of parameters effective and can show the flexibility of the model. When the prior is more concentrated, the regularized priors will decrease the flexibility.

## Q9.

```{r}
data(Laffer)
# data standardization
data(Laffer)
d <- Laffer
d$std_rate <- as.vector(scale(d$tax_rate))
d$std_revenue <- as.vector(scale(d$tax_revenue))
```

#### Model Fitting
```{r , message = FALSE, results='hide'}
# Linear model
mdl9.1 = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real a;
    real b1;
    real<lower = 0> sigma;
}
model {
    a ~ normal(0,0.2);
    b1 ~ normal(0,0.5);
    sigma ~ exponential(1);
    
    y ~ normal(a + x * b1, sigma);
}
generated quantities {
    vector[N] pred_mu;
    real pred_y[N];
    vector[N] log_lik; 
    
    pred_mu = a + b1 * x;
    pred_y = normal_rng(pred_mu, sigma);

    for (i in 1:N){
      log_lik[i] = normal_lpdf(y[i] | pred_mu[i], sigma);
    }
}
"
lm_data9.1 = list(N = nrow(d) , x = d$std_rate , y = d$std_revenue)
fit9.1 = stan(model_code = mdl9.1 , data = lm_data9.1 , iter = 2000 , chains = 2 , cores = 2)

# Quadratic
mdl9.2 = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real a;
    real b1;
    real b2;
    real<lower = 0> sigma;
}
model {
    a ~ normal(0,0.2);
    b1 ~ normal(0,0.5);
    b2 ~ normal(0,0.5);
    sigma ~ exponential(1);
    
    y ~ normal(a + x * b1 + x .* x * b2, sigma);
}
generated quantities {
    vector[N] pred_mu;
    real pred_y[N];
    vector[N] log_lik; 
    
    pred_mu = a + x * b1 + x .* x * b2;
    pred_y = normal_rng(pred_mu, sigma);

    for (i in 1:N){
      log_lik[i] = normal_lpdf(y[i] | pred_mu[i], sigma);
    }
}
"
lm_data9.2 = list(N = nrow(d) , x = d$std_rate , y = d$std_revenue)
fit9.2 = stan(model_code = mdl9.2 , data = lm_data9.2 , iter = 2000 , chains = 2 , cores = 2)

# Cubic
mdl9.3 = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real a;
    real b1;
    real b2;
    real b3;
    real<lower = 0> sigma;
}
model {
    a ~ normal(0,0.2);
    b1 ~ normal(0,0.5);
    b2 ~ normal(0,0.5);
    b3 ~ normal(0,0.5);
    sigma ~ exponential(1);
    
    y ~ normal(a + x * b1 + x .* x * b2 + x .* x .* x * b3, sigma);
}
generated quantities {
    vector[N] pred_mu;
    real pred_y[N];
    vector[N] log_lik; 
    
    pred_mu = a + x * b1 + x .* x * b2 + x .* x .* x * b3;
    pred_y = normal_rng(pred_mu, sigma);

    for (i in 1:N){
      log_lik[i] = normal_lpdf(y[i] | pred_mu[i], sigma);
    }
}
"
lm_data9.3 = list(N = nrow(d) , x = d$std_rate , y = d$std_revenue)
fit9.3 = stan(model_code = mdl9.3 , data = lm_data9.3 , iter = 2000 , chains = 2 , cores = 2)
```

#### Compare the models
```{r}
# Compare the results
rethinking::compare(fit9.1, fit9.2, fit9.3)
```

#### Plotting

由上面數據可看出 WAIC 和 Standard Deviation 很相近，但無法看出 Tax Rate 與 Revenue 的關係，故在下方 plot 出三個圖型關係。

```{r}
frame9.1 = as.data.frame(fit9.1)
frame9.2 = as.data.frame(fit9.2)
frame9.3 = as.data.frame(fit9.3)
pred_mu_1 = frame9.1 %>% select(contains("mu"))
pred_mu_2 = frame9.2 %>% select(contains("mu"))
pred_mu_3 = frame9.3 %>% select(contains("mu"))

PI = data.frame(
  rate = d$std_rate,
  mean1 = pred_mu_1 %>% apply(., 2, mean),
  mean2 = pred_mu_2 %>% apply(., 2, mean),
  mean3 = pred_mu_3 %>% apply(., 2, mean),
  L_HPDI1 = pred_mu_1 %>% apply(., 2 , PI , prob = 0.95) %>% .[1,],
  H_HPDI1 = pred_mu_1 %>% apply(., 2 , PI , prob = 0.95) %>% .[2,],
  L_HPDI2 = pred_mu_2 %>% apply(., 2 , PI , prob = 0.95) %>% .[1,],
  H_HPDI2 = pred_mu_2 %>% apply(., 2 , PI , prob = 0.95) %>% .[2,],
  L_HPDI3 = pred_mu_3 %>% apply(., 2 , PI , prob = 0.95) %>% .[1,],
  H_HPDI3 = pred_mu_3 %>% apply(., 2 , PI , prob = 0.95) %>% .[2,]
  )

PI %>% ggplot() +
  geom_point(data = d, aes(std_rate, std_revenue) , stroke=0 , size = 2.2) +
  geom_line(aes(rate , mean1 , colour = "Linear"), size = 1) +
  geom_ribbon(aes(x = rate , ymin = L_HPDI1 , ymax = H_HPDI1), alpha = 0.1) +
  geom_line(aes(rate , mean2 , colour = "Quadratic"), size = 1) +
  geom_ribbon(aes(x = rate , ymin = L_HPDI2 , ymax = H_HPDI2), alpha = 0.1) +
  geom_line(aes(rate , mean3 , colour = "Cubic"), size = 1) +
  geom_ribbon(aes(x = rate , ymin = L_HPDI3 , ymax = H_HPDI3), alpha = 0.1) +
  theme_bw() +
  labs(y = "Revenue", x = "Tax Rate") +
  scale_colour_discrete(name = "Model")
```

#### Interpretation

在 Linear Model 時只有顯示其正相關，但 Quadratic 和 Cubic Model 可逐漸看出趨勢圖與 Wall Street Journal 畫的趨勢相同，雖然曲線幅度沒有那麼高，但 Wall Street Journal 的 「the relationship between tax rates and tax revenue increases and then declines」的分析是正確的。

## Q10.

#### Delete Outlier and Fit Again

首先假設最高的那一點為此 data 的 outlier，將其刪除後再做一次 Q9 的模型。

```{r , message = FALSE, results='hide'}
# Delete the outlier
new_d = d[d$tax_revenue != max(d$tax_revenue), ]

# Linear model
mdl9.1_o = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real a;
    real b1;
    real<lower = 0> sigma;
}
model {
    a ~ normal(0,0.2);
    b1 ~ normal(0,0.5);
    sigma ~ exponential(1);
    
    y ~ normal(a + x * b1, sigma);
}
generated quantities {
    vector[N] pred_mu;
    real pred_y[N];
    vector[N] log_lik; 
    
    pred_mu = a + b1 * x;
    pred_y = normal_rng(pred_mu, sigma);

    for (i in 1:N){
      log_lik[i] = normal_lpdf(y[i] | pred_mu[i], sigma);
    }
}
"
lm_data9.1_o = list(N = nrow(new_d) , x = new_d$std_rate , y = new_d$std_revenue)
fit9.1_o = stan(model_code = mdl9.1_o , data = lm_data9.1_o , iter = 2000 , chains = 2 , cores = 2)

# Quadratic
mdl9.2_o = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real a;
    real b1;
    real b2;
    real<lower = 0> sigma;
}
model {
    a ~ normal(0,0.2);
    b1 ~ normal(0,0.5);
    b2 ~ normal(0,0.5);
    sigma ~ exponential(1);
    
    y ~ normal(a + x * b1 + x .* x * b2, sigma);
}
generated quantities {
    vector[N] pred_mu;
    real pred_y[N];
    vector[N] log_lik; 
    
    pred_mu = a + x * b1 + x .* x * b2;
    pred_y = normal_rng(pred_mu, sigma);

    for (i in 1:N){
      log_lik[i] = normal_lpdf(y[i] | pred_mu[i], sigma);
    }
}
"
lm_data9.2_o = list(N = nrow(new_d) , x = new_d$std_rate , y = new_d$std_revenue)
fit9.2_o = stan(model_code = mdl9.2_o , data = lm_data9.2_o , iter = 2000 , chains = 2 , cores = 2)

# Cubic
mdl9.3_o = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real a;
    real b1;
    real b2;
    real b3;
    real<lower = 0> sigma;
}
model {
    a ~ normal(0,0.2);
    b1 ~ normal(0,0.5);
    b2 ~ normal(0,0.5);
    b3 ~ normal(0,0.5);
    sigma ~ exponential(1);
    
    y ~ normal(a + x * b1 + x .* x * b2 + x .* x .* x * b3, sigma);
}
generated quantities {
    vector[N] pred_mu;
    real pred_y[N];
    vector[N] log_lik; 
    
    pred_mu = a + x * b1 + x .* x * b2 + x .* x .* x * b3;
    pred_y = normal_rng(pred_mu, sigma);

    for (i in 1:N){
      log_lik[i] = normal_lpdf(y[i] | pred_mu[i], sigma);
    }
}
"
lm_data9.3_o = list(N = nrow(new_d) , x = new_d$std_rate , y = new_d$std_revenue)
fit9.3_o = stan(model_code = mdl9.3_o , data = lm_data9.3_o , iter = 2000 , chains = 2 , cores = 2)
```

#### Robust regression 

**用 Student’s-t distribution 重新 fit model**

```{r , message = FALSE, results='hide'}
# Linear model
mdl9.1_t = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real a;
    real b1;
    real<lower = 0> sigma;
}
model {
    a ~ normal(0,0.2);
    b1 ~ normal(0,0.5);
    sigma ~ exponential(1);
    
    y ~ student_t(2 , a + x * b1, sigma);
}
generated quantities {
    vector[N] pred_mu;
    real pred_y[N];
    vector[N] log_lik; 
    
    pred_mu = a + b1 * x;
    pred_y = normal_rng(pred_mu, sigma);

    for (i in 1:N){
      log_lik[i] = normal_lpdf(y[i] | pred_mu[i], sigma);
    }
}
"
lm_data9.1_t = list(N = nrow(new_d) , x = new_d$std_rate , y = new_d$std_revenue)
fit9.1_t = stan(model_code = mdl9.1_t , data = lm_data9.1_t , iter = 2000 , chains = 2 , cores = 2)

# Quadratic
mdl9.2_t = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real a;
    real b1;
    real b2;
    real<lower = 0> sigma;
}
model {
    a ~ normal(0,0.2);
    b1 ~ normal(0,0.5);
    b2 ~ normal(0,0.5);
    sigma ~ exponential(1);
    
    y ~ student_t(2, a + x * b1 + x .* x * b2, sigma);
}
generated quantities {
    vector[N] pred_mu;
    real pred_y[N];
    vector[N] log_lik; 
    
    pred_mu = a + x * b1 + x .* x * b2;
    pred_y = normal_rng(pred_mu, sigma);

    for (i in 1:N){
      log_lik[i] = normal_lpdf(y[i] | pred_mu[i], sigma);
    }
}
"
lm_data9.2_t = list(N = nrow(new_d) , x = new_d$std_rate , y = new_d$std_revenue)
fit9.2_t = stan(model_code = mdl9.2_t , data = lm_data9.2_t , iter = 2000 , chains = 2 , cores = 2)

# Cubic
mdl9.3_t = "
data {
    int N; 
    vector[N] x;
    vector[N] y;
}
parameters {
    real a;
    real b1;
    real b2;
    real b3;
    real<lower = 0> sigma;
}
model {
    a ~ normal(0,0.2);
    b1 ~ normal(0,0.5);
    b2 ~ normal(0,0.5);
    b3 ~ normal(0,0.5);
    sigma ~ exponential(1);
    
    y ~ student_t(2, a + x * b1 + x .* x * b2 + x .* x .* x * b3, sigma);
}
generated quantities {
    vector[N] pred_mu;
    real pred_y[N];
    vector[N] log_lik; 
    
    pred_mu = a + x * b1 + x .* x * b2 + x .* x .* x * b3;
    pred_y = normal_rng(pred_mu, sigma);

    for (i in 1:N){
      log_lik[i] = normal_lpdf(y[i] | pred_mu[i], sigma);
    }
}
"
lm_data9.3_t = list(N = nrow(new_d) , x = new_d$std_rate , y = new_d$std_revenue)
fit9.3_t = stan(model_code = mdl9.3_t , data = lm_data9.3_t , iter = 2000 , chains = 2 , cores = 2)
```

#### Compare old model with new model

Since we can only compare models fit to exactly the same observations (the old data has 29 obs and the new data has 28 obs since we delete an outlier), I compare only the following two kinds of model.

```{r}
rethinking::compare(fit9.1_o, fit9.2_o, fit9.3_o, fit9.1_t, fit9.2_t, fit9.3_t)
```

**The model using Gaussian distribution with removed outlier performs the better than the student-t regression moel. We can also see the curve line below.**

```{r}
frame9.1_t = as.data.frame(fit9.1_t)
frame9.2_t = as.data.frame(fit9.2_t)
frame9.3_t = as.data.frame(fit9.3_t)
pred_mu_1_t = frame9.1_t %>% select(contains("mu"))
pred_mu_2_t = frame9.2_t %>% select(contains("mu"))
pred_mu_3_t = frame9.3_t %>% select(contains("mu"))

PI = data.frame(
  rate = d$std_rate,
  mean1 = pred_mu_1 %>% apply(., 2, mean),
  mean2 = pred_mu_2 %>% apply(., 2, mean),
  mean3 = pred_mu_3 %>% apply(., 2, mean),
  L_HPDI1 = pred_mu_1 %>% apply(., 2 , PI , prob = 0.95) %>% .[1,],
  H_HPDI1 = pred_mu_1 %>% apply(., 2 , PI , prob = 0.95) %>% .[2,],
  L_HPDI2 = pred_mu_2 %>% apply(., 2 , PI , prob = 0.95) %>% .[1,],
  H_HPDI2 = pred_mu_2 %>% apply(., 2 , PI , prob = 0.95) %>% .[2,],
  L_HPDI3 = pred_mu_3 %>% apply(., 2 , PI , prob = 0.95) %>% .[1,],
  H_HPDI3 = pred_mu_3 %>% apply(., 2 , PI , prob = 0.95) %>% .[2,]
)

PI %>% ggplot() +
  geom_point(data = new_d, aes(std_rate, std_revenue) , stroke=0 , size = 2.2) +
  geom_line(aes(rate , mean1 , colour = "Linear"), size = 1) +
  geom_ribbon(aes(x = rate , ymin = L_HPDI1 , ymax = H_HPDI1), alpha = 0.1) +
  geom_line(aes(rate , mean2 , colour = "Quadratic"), size = 1) +
  geom_ribbon(aes(x = rate , ymin = L_HPDI2 , ymax = H_HPDI2), alpha = 0.1) +
  geom_line(aes(rate , mean3 , colour = "Cubic"), size = 1) +
  geom_ribbon(aes(x = rate , ymin = L_HPDI3 , ymax = H_HPDI3), alpha = 0.1) +
  theme_bw() +
  labs(y = "Revenue", x = "Tax Rate") +
  scale_colour_discrete(name = "Model")
```

**Removing the outlier made our models' prediction intervals a little bit thinner. It is because the predictions are not influenced by the outlier anymore.**
